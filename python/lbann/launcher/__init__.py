import datetime
import os, os.path
import subprocess
import lbann
import lbann.proto
import lbann.launcher.flux
import lbann.launcher.openmpi
import lbann.launcher.slurm
import lbann.launcher.lsf
import lbann.launcher.pjm
from lbann.util import make_iterable, nvprof_command


def make_timestamped_work_dir(
    work_dir=None,
    experiment_dir=None,
    job_name='lbann',
    **kwargs,
):
    # Create work directory if not provided
    if not work_dir:
        work_dir = experiment_dir
    if not work_dir:
        if 'LBANN_EXPERIMENT_DIR' in os.environ:
            work_dir = os.environ['LBANN_EXPERIMENT_DIR']
        else:
            work_dir = os.path.join(os.getcwd())
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        work_dir = os.path.join(work_dir, '{}_{}'.format(timestamp, job_name))
        # Differentiate the work directory with a few key parameters
        if 'nodes' in kwargs:
            work_dir = ('{}_n{}'.format(work_dir, kwargs['nodes']))
        if 'procs_per_node' in kwargs:
            work_dir = ('{}_ppn{}'.format(work_dir, kwargs['procs_per_node']))

        i = 1
        while os.path.lexists(work_dir):
            i += 1
            work_dir = os.path.join(os.path.dirname(work_dir),
                                    '{}_{}_{}'.format(timestamp, job_name, i))
    work_dir = os.path.realpath(work_dir)
    os.makedirs(work_dir, exist_ok=True)

    return work_dir


# ==============================================
# Run experiments
# ==============================================


def run(
    trainer,
    model,
    data_reader,
    optimizer,
    work_dir=None,
    proto_file_name=None,
    nodes=1,
    procs_per_node=1,
    time_limit=None,
    scheduler=None,
    job_name='lbann',
    partition=None,
    account=None,
    reservation=None,
    launcher=None,
    launcher_args=[],
    lbann_exe=lbann.lbann_exe(),
    lbann_args=[],
    procs_per_trainer=None,
    environment={},
    preamble_cmds=[],
    overwrite_script=False,
    setup_only=False,
    batch_job=False,
    nvprof=False,
    nvprof_output_name=None,
    binary_protobuf=False,
    experiment_dir=None,
    profiler_cmd=None,
):
    """Run LBANN.

    This is intended to interface with job schedulers on HPC
    clusters. It will either submit a batch job (if on a login node)
    or run with an existing node allocation (if on a compute
    node). Behavior may vary across schedulers.

    If an experiment directory is not provided, a timestamped
    directory is created (by default in the current working
    directory). The location of autogenerated experiment directories
    can be set with the environment variable `LBANN_EXPERIMENT_DIR`.

    Args:
        trainer (lbann.Trainer): LBANN trainer.
        model (lbann.Model): Neural network model.
        data_reader (lbann.reader_pb2.DataReader): Data reader.
        optimizer (lbann.model.Optimizer): Default optimizer for
            model.
        work_dir (str, optional): Working directory.
        nodes (int, optional): Number of compute nodes.
        procs_per_node (int, optional): Number of processes per compute
            node.
        time_limit (int, optional): Job time limit, in minutes.
        scheduler (str, optional): Job scheduler.
        job_name (str, optional): Batch job name.
        partition (str, optional): Scheduler partition.
        account (str, optional): Scheduler account.
        reservation (str, optional): Scheduler reservation name.
        launcher (str, optional): Parallel command launcher.
        launcher_args (str, optional): Command-line arguments to
            launcher.
        lbann_exe (str, optional): LBANN executable.
        lbann_args (str, optional): Command-line arguments to LBANN
            executable.
        procs_per_trainer (int, optional): Number of processes per
            LBANN trainer. Default is all processes in one trainer.
        environment (dict of {str: str}, optional): Environment
            variables.
        overwrite_script (bool, optional): Whether to overwrite script
            file if it already exists.
        setup_only (bool, optional): If true, the experiment is not
            run after the experiment directory is initialized.
        batch_job (bool, optional): If true, the experiment is
            submitted to the scheduler as a batch job.
        nvprof (bool, optional): If true, an nvprof command is added
            to the beginning of LBANN executable.
        nvprof_output_name (str, optional): nvprof output filename.
            Filename should be unique to each process by using %q{ENV}
            (see https://docs.nvidia.com/cuda/profiler-users-guide/).
        binary_protobuf (bool, optional): If true, saves experiment description
            as a binary file. Otherwise, saves as prototext.
        experiment_dir (str, optional, deprecated): See `work_dir`.

    Returns:
        int: Exit status.

    """

    # Create batch script generator
    if not work_dir:
        work_dir = experiment_dir
    script = make_batch_script(work_dir=work_dir,
                               nodes=nodes,
                               procs_per_node=procs_per_node,
                               time_limit=time_limit,
                               scheduler=scheduler,
                               job_name=job_name,
                               partition=partition,
                               account=account,
                               reservation=reservation,
                               launcher=launcher,
                               launcher_args=launcher_args,
                               environment=environment,
                               preamble_cmds=preamble_cmds)

    # Batch script prints start time
    script.add_command('echo "Started at $(date)"')

    # Batch script invokes LBANN
    lbann_command = [lbann_exe]
    if nvprof:
        lbann_command = nvprof_command(
            work_dir=script.work_dir,
            output_name=nvprof_output_name) + lbann_command
    elif profiler_cmd is not None:
        lbann_command=[profiler_cmd]+lbann_command
    lbann_command.extend(make_iterable(lbann_args))

    # Set default file name and extension
    if proto_file_name is None:
        proto_file_name = ('experiment.protobin'
                           if binary_protobuf else 'experiment.prototext')
    proto_file = os.path.join(script.work_dir, proto_file_name)

    lbann.proto.save_prototext(proto_file,
                               binary=binary_protobuf,
                               trainer=trainer,
                               model=model,
                               data_reader=data_reader,
                               optimizer=optimizer)
    lbann_command.append('--prototext={}'.format(proto_file))
    if procs_per_trainer is not None:
        lbann_command.append(f'--procs_per_trainer={procs_per_trainer}')

    script.add_parallel_command(lbann_command)
    script.add_command('status=$?')

    # Batch script prints finish time and returns status
    script.add_command('echo "Finished at $(date)"')
    script.add_command('exit ${status}')

    # Write, submit, or run batch script
    status = 0
    if setup_only:
        script.write(overwrite=overwrite_script)
    elif batch_job:
        status = script.submit(overwrite=overwrite_script)
    else:
        status = script.run(overwrite=overwrite_script)
    return status


def make_batch_script(script_file=None,
                      work_dir=None,
                      nodes=1,
                      procs_per_node=1,
                      time_limit=None,
                      scheduler=None,
                      job_name='lbann',
                      partition=None,
                      account=None,
                      reservation=None,
                      launcher=None,
                      launcher_args=[],
                      environment={},
                      preamble_cmds=[],
                      cleanup_cmds=[],
                      experiment_dir=None):
    """Construct batch script manager.

    Attempts to detect a scheduler if one is not provided.

    If a working directory is not provided, a timestamped directory is
    created (by default in the current working directory). The
    location of autogenerated working directories can be set with the
    environment variable `LBANN_EXPERIMENT_DIR`.

    Args:
        script_file (str): Script file.
        work_dir (str, optional): Working directory
            (default: autogenerated, timestamped directory).
        nodes (int, optional): Number of compute nodes
            (default: 1).
        procs_per_node (int, optional): Parallel processes per
            compute node (default: 1).
        time_limit (int, optional): Job time limit, in minutes.
        scheduler (str, optional): Job scheduler
            (default: autodetected scheduler).
        job_name (str, optional): Job name (default: 'lbann').
        partition (str, optional): Scheduler partition.
        account (str, optional): Scheduler account.
        reservation (str, optional): Scheduler advance reservation.
        launcher (str, optional): Parallel command launcher.
        launcher_args (`Iterable` of `str`, optional):
            Command-line arguments to parallel command launcher.
        environment (`dict` of `{str: str}`, optional): Environment
            variables.
        experiment_dir (str, optional, deprecated): See `work_dir`.

    Returns:
        `lbann.launcher.batch_script.BatchScript`

    """

    # Try detecting job scheduler if not provided
    # Note: Fallback to OpenMPI launcher
    if not scheduler:
        try:
            subprocess.call(['sbatch', '--version'],
                            stdout=subprocess.DEVNULL,
                            stderr=subprocess.DEVNULL)
            scheduler = 'slurm'
        except:
            pass
    if not scheduler:
        try:
            subprocess.call(['bsub', '-V'],
                            stdout=subprocess.DEVNULL,
                            stderr=subprocess.DEVNULL)
            scheduler = 'lsf'
        except:
            pass
    if not scheduler:
        try:
            subprocess.call(['flux', '-V'],
                            stdout=subprocess.DEVNULL,
                            stderr=subprocess.DEVNULL)
            scheduler = 'flux'
            print('I have found a flux scheduler')
        except:
            pass
    if not scheduler:
        scheduler = 'openmpi'

    # Create work directory if not provided
    work_dir = make_timestamped_work_dir(work_dir=work_dir,
                                         experiment_dir=experiment_dir,
                                         job_name=job_name,
                                         nodes=nodes,
                                         procs_per_node=procs_per_node)

    # Create batch script manager
    if not script_file:
        script_file = os.path.join(work_dir, 'batch.sh')
    script = None
    if scheduler.lower() in ('openmpi', ):
        script = lbann.launcher.openmpi.OpenMPIBatchScript(
            script_file=script_file,
            work_dir=work_dir,
            nodes=nodes,
            procs_per_node=procs_per_node,
            launcher=launcher,
            launcher_args=launcher_args)
    elif scheduler.lower() in ('slurm', 'srun', 'sbatch'):
        script = lbann.launcher.slurm.SlurmBatchScript(
            script_file=script_file,
            work_dir=work_dir,
            nodes=nodes,
            procs_per_node=procs_per_node,
            time_limit=time_limit,
            job_name=job_name,
            partition=partition,
            account=account,
            launcher=launcher,
            launcher_args=launcher_args)
    elif scheduler.lower() in ('flux'):
        script = lbann.launcher.flux.FluxBatchScript(
            script_file=script_file,
            work_dir=work_dir,
            nodes=nodes,
            procs_per_node=procs_per_node,
            time_limit=time_limit,
            job_name=job_name,
            partition=partition,
            account=account,
            launcher=launcher,
            launcher_args=launcher_args)
    elif scheduler.lower() in ('lsf', 'jsrun', 'bsub'):
        script = lbann.launcher.lsf.LSFBatchScript(
            script_file=script_file,
            work_dir=work_dir,
            nodes=nodes,
            procs_per_node=procs_per_node,
            time_limit=time_limit,
            job_name=job_name,
            partition=partition,
            account=account,
            reservation=reservation,
            launcher=launcher,
            launcher_args=launcher_args)
    elif scheduler.lower() in ('pjm', 'pjsub'):
        script = lbann.launcher.pjm.PJMBatchScript(
            script_file=script_file,
            work_dir=work_dir,
            nodes=nodes,
            procs_per_node=procs_per_node,
            time_limit=time_limit,
            job_name=job_name,
            partition=partition,
            launcher=launcher,
            launcher_args=launcher_args)
    else:
        raise RuntimeError('unsupported job scheduler ({})'.format(scheduler))

    # Set batch script preamble commands
    for cmd in preamble_cmds:
        script.add_command(cmd)

    # Set batch script environment
    for variable, value in environment.items():
        script.add_command('export {0}={1}'.format(variable, value))

    # Set batch script cleanup commmands
    for cmd in cleanup_cmds:
        script.add_command(cmd)

    return script
